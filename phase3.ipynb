{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2577bd3a-9834-43bf-9033-d7470026048c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plant_name</th>\n",
       "      <th>ab</th>\n",
       "      <th>ak</th>\n",
       "      <th>al</th>\n",
       "      <th>ar</th>\n",
       "      <th>az</th>\n",
       "      <th>bc</th>\n",
       "      <th>ca</th>\n",
       "      <th>co</th>\n",
       "      <th>ct</th>\n",
       "      <th>...</th>\n",
       "      <th>tx</th>\n",
       "      <th>ut</th>\n",
       "      <th>va</th>\n",
       "      <th>vi</th>\n",
       "      <th>vt</th>\n",
       "      <th>wa</th>\n",
       "      <th>wi</th>\n",
       "      <th>wv</th>\n",
       "      <th>wy</th>\n",
       "      <th>yt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abelia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abelia x grandiflora</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abelmoschus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abelmoschus esculentus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abelmoschus moschatus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               plant_name  ab  ak  al  ar  az  bc  ca  co  ct  ...  tx  ut  \\\n",
       "0                  abelia   0   0   0   0   0   0   0   0   0  ...   0   0   \n",
       "1    abelia x grandiflora   0   0   0   0   0   0   0   0   0  ...   0   0   \n",
       "2             abelmoschus   0   0   0   0   0   0   0   0   1  ...   0   0   \n",
       "3  abelmoschus esculentus   0   0   0   0   0   0   0   0   1  ...   0   0   \n",
       "4   abelmoschus moschatus   0   0   0   0   0   0   0   0   0  ...   0   0   \n",
       "\n",
       "   va  vi  vt  wa  wi  wv  wy  yt  \n",
       "0   0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0   0  \n",
       "2   1   1   0   0   0   0   0   0  \n",
       "3   1   1   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .data file\n",
    "file_path = \"plants/plants.data\"\n",
    "\n",
    "# Read and parse the file\n",
    "with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "# Split into plant name and states\n",
    "data = []\n",
    "for line in raw_lines:\n",
    "    parts = line.strip().split(\",\")\n",
    "    plant_name = parts[0]\n",
    "    states = parts[1:]  # list of state abbreviations\n",
    "    data.append((plant_name, states))\n",
    "\n",
    "# Create a DataFrame from the parsed data\n",
    "df = pd.DataFrame(data, columns=[\"plant_name\", \"states\"])\n",
    "\n",
    "# Convert state list into binary columns (one-hot encoding)\n",
    "# Explode the list of states into rows, then pivot to binary columns\n",
    "df_exploded = df.explode(\"states\")\n",
    "df_encoded = pd.crosstab(df_exploded.index, df_exploded[\"states\"])\n",
    "\n",
    "# Combine with the plant name (optional)\n",
    "df_final = pd.concat([df[\"plant_name\"], df_encoded], axis=1)\n",
    "\n",
    "# Show the final DataFrame\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d701e5d1-2635-437a-8c94-c57b3dd3514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aruno\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9345 - loss: 0.1773 - val_accuracy: 1.0000 - val_loss: 2.7461e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0012 - val_accuracy: 0.9998 - val_loss: 2.2901e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.1448e-04 - val_accuracy: 1.0000 - val_loss: 7.2322e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 5.4364e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6273e-04 - val_accuracy: 1.0000 - val_loss: 2.9093e-06\n",
      "Epoch 6/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9989 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 7.9542e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 3.2945e-04 - val_accuracy: 1.0000 - val_loss: 2.7222e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 5.7477e-04 - val_accuracy: 1.0000 - val_loss: 1.0235e-06\n",
      "Epoch 9/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0198e-05 - val_accuracy: 1.0000 - val_loss: 8.3860e-07\n",
      "Epoch 10/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.4679e-06 - val_accuracy: 1.0000 - val_loss: 6.0047e-07\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.3910e-07  \n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Drop plant name, get feature matrix (X)\n",
    "X = df_final.drop(columns=[\"plant_name\"]).values\n",
    "\n",
    "# For learning purpose, let's try to predict ONE of the states (e.g., 'fl') just as binary classification\n",
    "# You can change 'fl' to any state that appears often\n",
    "y = df_final[\"fl\"].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the MLP model\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X.shape[1],), activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11973327-01d4-41d2-9dd5-dbe4362a54d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2413 - loss: 0.2904 - val_accuracy: 0.4952 - val_loss: 0.0749\n",
      "Epoch 2/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4742 - loss: 0.0886 - val_accuracy: 0.4996 - val_loss: 0.0576\n",
      "Epoch 3/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.4936 - loss: 0.0747 - val_accuracy: 0.4988 - val_loss: 0.0504\n",
      "Epoch 4/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4929 - loss: 0.0689 - val_accuracy: 0.5157 - val_loss: 0.0460\n",
      "Epoch 5/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4919 - loss: 0.0653 - val_accuracy: 0.5039 - val_loss: 0.0433\n",
      "Epoch 6/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5006 - loss: 0.0617 - val_accuracy: 0.5157 - val_loss: 0.0421\n",
      "Epoch 7/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5035 - loss: 0.0595 - val_accuracy: 0.5111 - val_loss: 0.0396\n",
      "Epoch 8/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4920 - loss: 0.0599 - val_accuracy: 0.4893 - val_loss: 0.0385\n",
      "Epoch 9/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5047 - loss: 0.0583 - val_accuracy: 0.5112 - val_loss: 0.0369\n",
      "Epoch 10/10\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4933 - loss: 0.0579 - val_accuracy: 0.5093 - val_loss: 0.0365\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5125 - loss: 0.0343 \n",
      "Multi-label Accuracy: 0.5160\n"
     ]
    }
   ],
   "source": [
    "# Prepare the full label matrix (all states, excluding plant name)\n",
    "y_multi = df_final.drop(columns=[\"plant_name\"]).values\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build multi-label MLP\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "x = Dense(128, activation='relu')(input_layer)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "output_layer = Dense(y_multi.shape[1], activation='sigmoid')(x)  # One sigmoid per label\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile with binary crossentropy for multi-label\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Multi-label Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38e1129-6660-4942-9f7a-f5da7b8ca96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step  \n",
      "Hamming Loss: 0.012782603338877594\n",
      "F1 Score (micro): 0.9480856316039663\n",
      "F1 Score (macro): 0.9341247300056909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
    "print(\"F1 Score (micro):\", f1_score(y_test, y_pred, average='micro'))\n",
    "print(\"F1 Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d17e7f-3b10-4445-adcd-8b48bbccb0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"mlp_environmental_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05764b0-0b2c-4827-bce5-4d4ecf88ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
